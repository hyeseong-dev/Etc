{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitpytorchcondae2e836418e8f4945879c9669572061e2",
   "display_name": "Python 3.8.1 64-bit ('pytorch': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "123456789/n"
    }
   ],
   "source": [
    "url = '123'+'456'+'789'/n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Cannot find load more button.../nCannot find load more button.../nCannot find load more button.../nCannot find load more button.../nCannot find load more button.../nCannot find load more button.../nCannot find load more button.../nCannot find load more button.../nCannot find load more button.../nCannot find load more button.../nThere are 200 reviews avaliable!/nWriting the data.../nDone!/n"
    }
   ],
   "source": [
    "from selenium import webdriver/n",
    "from bs4 import BeautifulSoup/n",
    "import time, os/n",
    "from datetime import datetime/n",
    "import pandas as pd/n",
    "/n",
    "# 리뷰할 링크/n",
    "link = 'https://play.google.com/store/apps/details?id=com.miso&hl=ko&showAllReviews=true'/n",
    "/n",
    "# 스크롤을 몇번 내릴래요?/n",
    "scroll_cnt = 10/n",
    "/n",
    "# chrome driver 다운 받아야 바로 아래 코드 2줄 사용가능 https://sites.google.com/a/chromium.org/chromedriver/home/n",
    "driver = webdriver.Chrome('./chromedriver') # webdriver패키지에서 chorome driver를 사용함(매개변수는 chromedirver의 위치를 말함)/n",
    "driver.get(link)                            # 링크를 가져온다(실제 크롬이 켜져서 실행됨.)/n",
    "/n",
    "os.makedirs('result', exist_ok=True)        # 'result' 디렉토리 생성, 파이썬3.2부터 디렉토리내 해당명칭의 디렉토리가 없으면 안전히 생성시킴/n",
    "/n",
    "/n",
    "/n",
    "/n",
    "/n",
    "# 아래 for문은 스코롤을 10번 내리면서 명령을 수행하는 코드를 짠거./n",
    "# 1.스크롤을 내리는 기능 javascript의 스크롤 내리는 이벤트 기능을 가져옴. /n",
    "# 2. datetime 패키지에서 sleep메소드를 통해 3초동안 정지한 다음 다음 코드로 실행을 이어감. /n",
    "# 3. try(참)인 경우는 더보기 버튼을 찾게 한다.(XML문서에서 노드의 위치를 찾을때 사용)/n",
    "#    except(거짓) 더보기 버튼 없이 그냥 스크롤바 내리면 자동으로 정보를 불러낸다면 오류 없이 실행시키기 위한 코드임./n",
    "/n",
    "for i in range(scroll_cnt):/n",
    "  # scroll to bottom/n",
    "  driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')# 0~document.body.scrollHeight(웹의 바디(몸통)크기만큼) 스크롤해/n",
    "  time.sleep(3)                                                           # 여기서 3초만 쉬자!/n",
    "/n",
    "/n",
    "  try:/n",
    "    #xml문서에서 더보기 버튼을 찾아서 클릭하게 만드는 코드입니다. /n",
    "    load_more = driver.find_element_by_xpath('//*[contains(@class,/"U26fgb O0WRkf oG5Srb C0oVfc n9lfJ/")]').click()/n",
    "    /n",
    "  except:/n",
    "    print('Cannot find load more button...')/n",
    "/n",
    "# get review containers/n",
    "reviews = driver.find_elements_by_xpath('//*[@jsname=/"fk8dgd/"]//div[@class=/"d15Mdf bAhLNe/"]')/n",
    "/n",
    "print('There are %d reviews avaliable!' % len(reviews))/n",
    "print('Writing the data...')/n",
    "/n",
    "# create empty dataframe to store data/n",
    "df = pd.DataFrame(columns=['name', 'rating', 'date', 'likes', 'comment', 'dlp_comment'])/n",
    "/n",
    "# get review data/n",
    "for review in reviews:/n",
    "  # parse string to html using bs4/n",
    "    soup = BeautifulSoup(review.get_attribute('innerHTML'), 'html.parser')/n",
    "/n",
    "  # reviewer/n",
    "    name= soup.find(class_='X43Kjb').text/n",
    "/n",
    "  # rating/n",
    "    rating= int(soup.find('div', role='img').get('aria-label').replace('별표 5개 만점에', '').replace('개를 받았습니다.', '').strip())/n",
    "/n",
    "  # review date/n",
    "    date= soup.find(class_='p2TkOb').text/n",
    "    date= datetime.strptime(date, '%Y년 %m월 %d일')/n",
    "    date= date.strftime('%Y-%m-%d')/n",
    "/n",
    "  # helpful/n",
    "    likes= soup.find(class_='jUL89d y92BAb').text/n",
    "    if not likes:/n",
    "        likes= 0/n",
    "  /n",
    "  # review text/n",
    "    comment= soup.find('span', jsname='fbQN7e').text/n",
    "    if not comment:/n",
    "        comment= soup.find('span', jsname='bN97Pc').text/n",
    "  /n",
    "  # developer comment/n",
    "    dlp_comment= None/n",
    "    dc_div = soup.find('div', class_='LVQB0b')/n",
    "    if dc_div:/n",
    "        dlp_comment= dc_div.text.replace('//n', ' ')/n",
    "  /n",
    "  # append to dataframe/n",
    "    df = df.append({/n",
    "    'name': name,/n",
    "    'rating': rating,/n",
    "    'date': date,/n",
    "    'likes': likes,/n",
    "    'comment': comment,/n",
    "    'dlp_comment': dlp_comment/n",
    "    }, ignore_index=True)/n",
    "/n",
    "# finally save the dataframe into csv file/n",
    "filename = datetime.now().strftime('result/%Y-%m-%d_%H-%M-%S.csv')/n",
    "df.to_csv(filename, encoding='utf-8-sig', index=False)/n",
    "driver.stop_client()/n",
    "driver.close()/n",
    "/n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver/n",
    "from bs4 import BeautifulSoup/n",
    "import time, os/n",
    "from datetime import datetime/n",
    "import pandas as pd/n",
    "/n",
    "# 리뷰할 링크/n",
    "link = 'https://play.google.com/store/apps/details?id=com.miso&hl=ko&showAllReviews=true'/n",
    "/n",
    "# 스크롤을 몇번 내릴래요?/n",
    "scroll_cnt = 10/n",
    "/n",
    "# chrome driver 다운 받아야 바로 아래 코드 2줄 사용가능 https://sites.google.com/a/chromium.org/chromedriver/home/n",
    "driver = webdriver.Chrome('./chromedriver') # webdriver패키지에서 chorome driver를 사용함(매개변수는 chromedirver의 위치를 말함)/n",
    "driver.get(link)                            # 링크를 가져온다(실제 크롬이 켜져서 실행됨.)/n",
    "/n",
    "os.makedirs('result', exist_ok=True)        # 'result' 디렉토리 생성, 파이썬3.2부터 디렉토리내 해당명칭의 디렉토리가 없으면 안전히 생성시킴/n",
    "/n",
    "/n",
    "/n",
    "/n",
    "/n",
    "# 아래 for문은 스코롤을 10번 내리면서 명령을 수행하는 코드를 짠거./n",
    "# 1.스크롤을 내리는 기능 javascript의 스크롤 내리는 이벤트 기능을 가져옴. /n",
    "# 2. datetime 패키지에서 sleep메소드를 통해 3초동안 정지한 다음 다음 코드로 실행을 이어감. /n",
    "# 3. try(참)인 경우는 더보기 버튼을 찾게 한다.(XML문서에서 노드의 위치를 찾을때 사용)/n",
    "#    except(거짓) 더보기 버튼 없이 그냥 스크롤바 내리면 자동으로 정보를 불러낸다면 오류 없이 실행시키기 위한 코드임./n",
    "/n",
    "for i in range(scroll_cnt):/n",
    "  # scroll to bottom/n",
    "  driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')# 0~document.body.scrollHeight(웹의 바디(몸통)크기만큼) 스크롤해/n",
    "  time.sleep(3)                                                           # 여기서 3초만 쉬자!/n",
    "/n",
    "/n",
    "  try:/n",
    "    #xml문서에서 더보기 버튼을 찾아서 클릭하게 만드는 코드입니다. *는 어떠한거든(문자,숫자)다 포함된다는 거고 contain은 몰라./n",
    "    load_more = driver.find_element_by_xpath('//*[contains(@class,/"U26fgb O0WRkf oG5Srb C0oVfc n9lfJ/")]').click()/n",
    "    /n",
    "  except:/n",
    "    print('Cannot find load more button...')/n",
    "/n",
    "# get review containers/n",
    "reviews = driver.find_elements_by_xpath('//*[@jsname=/"fk8dgd/"]//div[@class=/"d15Mdf bAhLNe/"]')/n",
    "/n",
    "print('There are %d reviews avaliable!' % len(reviews))/n",
    "print('Writing the data...')/n",
    "/n",
    "# create empty dataframe to store data/n",
    "df = pd.DataFrame(columns=['name', 'rating', 'date', 'likes', 'comment', 'dlp_comment'])/n",
    "/n",
    "# get review data/n",
    "for review in reviews:/n",
    "  # parse string to html using bs4/n",
    "    soup = BeautifulSoup(review.get_attribute('innerHTML'), 'html.parser')/n",
    "/n",
    "  # reviewer/n",
    "    name= soup.find(class_='X43Kjb').text/n",
    "/n",
    "  # rating/n",
    "    rating= int(soup.find('div', role='img').get('aria-label').replace('별표 5개 만점에', '').replace('개를 받았습니다.', '').strip())/n",
    "/n",
    "  # review date/n",
    "    date= soup.find(class_='p2TkOb').text/n",
    "    date= datetime.strptime(date, '%Y년 %m월 %d일')/n",
    "    date= date.strftime('%Y-%m-%d')/n",
    "/n",
    "  # helpful/n",
    "    likes= soup.find(class_='jUL89d y92BAb').text/n",
    "    if not likes:/n",
    "        likes= 0/n",
    "  /n",
    "  # review text/n",
    "    comment= soup.find('span', jsname='fbQN7e').text/n",
    "    if not comment:/n",
    "        comment= soup.find('span', jsname='bN97Pc').text/n",
    "  /n",
    "  # developer comment/n",
    "    dlp_comment= None/n",
    "    dc_div = soup.find('div', class_='LVQB0b')/n",
    "    if dc_div:/n",
    "        dlp_comment= dc_div.text.replace('//n', ' ')/n",
    "  /n",
    "  # append to dataframe/n",
    "    df = df.append({/n",
    "    'name': name,/n",
    "    'rating': rating,/n",
    "    'date': date,/n",
    "    'likes': likes,/n",
    "    'comment': comment,/n",
    "    'dlp_comment': dlp_comment/n",
    "    }, ignore_index=True)/n",
    "/n",
    "# finally save the dataframe into csv file/n",
    "filename = datetime.now().strftime('result/%Y-%m-%d_%H-%M-%S.csv')/n",
    "df.to_csv(filename, encoding='utf-8-sig', index=False)/n",
    "driver.stop_client()/n",
    "driver.close()/n",
    "/n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver/n",
    "from bs4 import BeautifulSoup/n",
    "import time, os/n",
    "from datetime import datetime/n",
    "import pandas as pd/n",
    "/n",
    "# review link link/n",
    "link = 'https://play.google.com/store/apps/details?id=com.miso&hl=ko&showAllReviews=true'/n",
    "/n",
    "# how many scrolls we need/n",
    "scroll_cnt = 10/n",
    "/n",
    "# download chrome driver https://sites.google.com/a/chromium.org/chromedriver/home/n",
    "driver = webdriver.Chrome('./chromedriver')/n",
    "driver.get(link)/n",
    "/n",
    "os.makedirs('result', exist_ok=True)/n",
    "/n",
    "for i in range(scroll_cnt):/n",
    "  # scroll to bottom/n",
    "  driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')/n",
    "  time.sleep(3)/n",
    "/n",
    "  # click 'Load more' button, if exists/n",
    "  try:/n",
    "    load_more = driver.find_element_by_xpath('//*[contains(@class,/"U26fgb O0WRkf oG5Srb C0oVfc n9lfJ/")]').click()/n",
    "  except:/n",
    "    print('Cannot find load more button...')/n",
    "/n",
    "# get review containers/n",
    "reviews = driver.find_elements_by_xpath('//*[@jsname=/"fk8dgd/"]//div[@class=/"d15Mdf bAhLNe/"]')/n",
    "/n",
    "print('There are %d reviews avaliable!' % len(reviews))/n",
    "print('Writing the data...')/n",
    "/n",
    "# create empty dataframe to store data/n",
    "df = pd.DataFrame(columns=['', '', '', '', '', ''])/n",
    "/n",
    "# get review data/n",
    "for review in reviews:/n",
    "  # parse string to html using bs4/n",
    "  soup = BeautifulSoup(review.get_attribute('innerHTML'), 'html.parser')/n",
    "/n",
    "  # reviewer/n",
    "   = soup.find(class_='X43Kjb').text/n",
    "/n",
    "  # rating/n",
    "   = int(soup.find('div', role='img').get('aria-label').replace('별표 5개 만점에', '').replace('개를 받았습니다.', '').strip())/n",
    "/n",
    "  # review date/n",
    "   = soup.find(class_='p2TkOb').text/n",
    "   = datetime.strptime(, '%Y년 %m월 %d일')/n",
    "   = date.strftime('%Y-%m-%d')/n",
    "/n",
    "  # helpful/n",
    "   = soup.find(class_='jUL89d y92BAb').text/n",
    "  if not :/n",
    "     = 0/n",
    "  /n",
    "  # review text/n",
    "   = soup.find('span', jsname='fbQN7e').text/n",
    "  if not :/n",
    "     = soup.find('span', jsname='bN97Pc').text/n",
    "  /n",
    "  # developer comment/n",
    "   = None/n",
    "  dc_div = soup.find('div', class_='LVQB0b')/n",
    "  if dc_div:/n",
    "     = dc_div.text.replace('//n', ' ')/n",
    "  /n",
    "  # append to dataframe/n",
    "  df = df.append({/n",
    "    '': ,/n",
    "    '': ,/n",
    "    '': ,/n",
    "    '': ,/n",
    "    '': ,/n",
    "    '': /n",
    "  }, ignore_index=True)/n",
    "/n",
    "# finally save the dataframe into csv file/n",
    "filename = datetime.now().strftime('result/%Y-%m-%d_%H-%M-%S.csv')/n",
    "df.to_csv(filename, encoding='utf-8-sig', index=False)/n",
    "driver.stop_client()/n",
    "driver.close()/n",
    "/n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver/n",
    "from bs4 import BeautifulSoup/n",
    "import time, os/n",
    "from datetime import datetime/n",
    "import pandas as pd/n",
    "/n",
    "# review link link/n",
    "link = 'https://play.google.com/store/apps/details?id=com.miso&hl=ko&showAllReviews=true'/n",
    "/n",
    "# how many scrolls we need/n",
    "scroll_cnt = 10/n",
    "/n",
    "# download chrome driver https://sites.google.com/a/chromium.org/chromedriver/home/n",
    "driver = webdriver.Chrome('./chromedriver')/n",
    "driver.get(link)/n",
    "/n",
    "os.makedirs('result', exist_ok=True)/n",
    "/n",
    "for i in range(scroll_cnt):/n",
    "  # scroll to bottom/n",
    "  driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')/n",
    "  time.sleep(3)/n",
    "/n",
    "  # click 'Load more' button, if exists/n",
    "  try:/n",
    "    load_more = driver.find_element_by_xpath('//*[contains(@class,/"U26fgb O0WRkf oG5Srb C0oVfc n9lfJ/")]').click()/n",
    "  except:/n",
    "    print('Cannot find load more button...')/n",
    "/n",
    "# get review containers/n",
    "reviews = driver.find_elements_by_xpath('//*[@jsname=/"fk8dgd/"]//div[@class=/"d15Mdf bAhLNe/"]')/n",
    "/n",
    "print('There are %d reviews avaliable!' % len(reviews))/n",
    "print('Writing the data...')/n",
    "/n",
    "# create empty dataframe to store data/n",
    "df = pd.DataFrame(columns=['', '', '', '', '', ''])/n",
    "/n",
    "# get review data/n",
    "for review in reviews:/n",
    "  # parse string to html using bs4/n",
    "  soup = BeautifulSoup(review.get_attribute('innerHTML'), 'html.parser')/n",
    "/n",
    "  # reviewer/n",
    "   = soup.find(class_='X43Kjb').text/n",
    "/n",
    "  # rating/n",
    "  ratings = int(soup.find('div', role='img').get('aria-label').replace('별표 5개 만점에', '').replace('개를 받았습니다.', '').strip())/n",
    "/n",
    "  # review date/n",
    "   = soup.find(class_='p2TkOb').text/n",
    "   = datetime.strptime(, '%Y년 %m월 %d일')/n",
    "   = date.strftime('%Y-%m-%d')/n",
    "/n",
    "  # helpful/n",
    "  helpful = soup.find(class_='jUL89d y92BAb').text/n",
    "  if not helpful:/n",
    "    helpful = 0/n",
    "  /n",
    "  # review text/n",
    "   = soup.find('span', jsname='fbQN7e').text/n",
    "  if not :/n",
    "     = soup.find('span', jsname='bN97Pc').text/n",
    "  /n",
    "  # developer comment/n",
    "   = None/n",
    "  dc_div = soup.find('div', class_='LVQB0b')/n",
    "  if dc_div:/n",
    "     = dc_div.text.replace('//n', ' ')/n",
    "  /n",
    "  # append to dataframe/n",
    "  df = df.append({/n",
    "    '': ,/n",
    "    '': ,/n",
    "    '': ,/n",
    "    '': ,/n",
    "    '': ,/n",
    "    '': /n",
    "  }, ignore_index=True)/n",
    "/n",
    "# finally save the dataframe into csv file/n",
    "filename = datetime.now().strftime('result/%Y-%m-%d_%H-%M-%S.csv')/n",
    "df.to_csv(filename, encoding='utf-8-sig', index=False)/n",
    "driver.stop_client()/n",
    "driver.close()/n",
    "/n",
    "print('Done!')"
   ]
  }
 ]
}